{%- set osd_pairs = [] %}
{%- for host in groups['osd-nodes'] %}
{%- set host_osd_jour = hostvars[host]['osd_jour'] %}
{%- set class = namespace(area="") %}
{%- if join_areas.enabled %}
{%- for area in join_areas.areas %}
{%- if area.name+'-osd-node' in hostvars[host].group_names %}
{%- set class.area = area.name %}
{%- endif %}
{%- endfor %}
{%- endif %}
{%- set disks = hostvars[host]['ansible_local']['osd_disks']|difference([host_osd_jour]) %}
{%- for disk in disks %}
{%- do osd_pairs.append(disk+"-"+host_osd_jour+"-"+class.area) %}
{%- endfor %}
{%- endfor %}
---
endpoints:
  identity:
    namespace: openstack
  object_store:
    namespace: ceph
  ceph_mon:
    namespace: ceph
deployment:
  storage_secrets: true
  ceph: true
  rbd_provisioner: true
  cephfs_provisioner: false
  client_secrets: false
  rgw_keystone_user_and_endpoints: false

bootstrap:
  enabled: true

storageclass:
  rbd:
    pool: cloud.infra
  cephfs:
    provision_storage_class: false

conf:
  features:
    mds: false
  ceph:
    global:
      fsid: "{{ceph_fs_id}}"
      bluestore_block_db_size: "{{bluestore_block_db_size}}"
      bluestore_block_wal_size: "{{bluestore_block_wal_size}}"
      mon osd full ratio: "{{mon_osd_full_ratio}}"
      mon osd nearfull ratio: "{{mon_osd_nearfull_ratio}}"
      objecter_inflight_op_bytes: "1073741824"
      objecter_inflight_ops: 10240
      debug_ms: "0/0"
    osd:
      osd_mkfs_type: xfs
      osd_mkfs_options_xfs: -f -i size=2048
      osd_max_object_name_len: 256
      ms_bind_port_min: 6800
      ms_bind_port_max: 7100
      osd_snap_trim_priority: 1
      osd_snap_trim_sleep: 0.1
      osd_pg_max_concurrent_snap_trims: 1
      filestore_merge_threshold: -10
      filestore_split_multiple: 12
      osd_scrub_begin_hour: 22
      osd_scrub_end_hour: 4
      osd_scrub_during_recovery: false
      osd_scrub_sleep: 0.1
      osd_scrub_chunk_min: 1
      osd_scrub_chunk_max: 4
      osd_deep_scrub_stride: "1048576"
      osd_scrub_priority: 1
      osd_recovery_op_priority: 1
      osd_recovery_max_active: 1
      osd_mount_options_xfs: "rw,noatime,largeio,inode64,swalloc,logbufs=8,logbsize=256k,allocsize=4M"
  rgw_ks:
    enabled: true
  pool:
    crush:
      tunables: {{tunables}}
    target:
      osd: 12
      pg_per_osd: {{pg_per_osd}}
    spec:
      # RBD pool
      - name: {{infra.pool_name}}
        application: rbd
        replication: {{replication}}
        percent_total_data: {{infra_pool_percent}}
      # Cinder pools
{% if join_areas.enabled %}
{% for area in join_areas.areas %}
      - name: {{cinder.pool_name}}.{{area.name}}
        application: cinder-volume
        replication: {{replication}}
        percent_total_data: {{area.cinder_pool_percent}}
        crush_rule: {{area.name}}_rule
{% endfor %}
{% else %}
      - name: {{cinder.pool_name}}
        application: cinder-volume
        replication: {{replication}}
        percent_total_data: {{cinder_pool_percent}}
{% endif %}
      - name: {{cinder.backup_pool_name}}
        application: cinder-backup
        replication: {{replication}}
        percent_total_data: {{cinder_backup_pool_percent}}
      # Glance pool
      - name: {{glance.pool_name}}
        application: glance-image
        replication: {{replication}}
        percent_total_data: {{glance_pool_percent}}
      # Nova ephimeral pool
      - name: {{nova.ephimeral_pool_name}}
        application: nova
        replication: {{replication}}
        percent_total_data: {{nova_ephimeral_pool_percent}}
      # RadosGW pools
      - name: .rgw.root
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.control
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.data.root
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.gc
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.log
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.intent-log
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.meta
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.usage
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.keys
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.email
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.swift
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.uid
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.buckets.extra
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.buckets.index
        application: rgw
        replication: {{replication}}
        percent_total_data: 3
      - name: default.rgw.buckets.data
        application: rgw
        replication: {{replication}}
        percent_total_data: {{rgw_pool_percent}}

  storage:
    mon:
      directory: /var/lib/ceph/mon
  overrides:
    ceph_osd:
      labels:
{% for osd in osd_pairs|unique %}
{% set params = osd.split('-') %}
       - label: 
{% if join_areas.enabled %}
           key: d{{params[0]}}-j{{params[1]}}-{{params[2]}}
{% else %}
           key: d{{params[0]}}-j{{params[1]}}
{% endif %}
           values: 
           - "enabled"
         #exclusive: false
         conf:
           storage:
             osd:
               - data:
                   type: block-bluestore
                   location: /dev/{{params[0]}}
                 journal:
                   type: block-bluestore
                   location: /dev/{{params[1]}}
{% endfor %}
ceph_mgr_enabled_modules:
  - restful
  - status
  - prometheus
  - balancer
# Manager default port conflict with cassandra(TungstenFabric)
ceph_mgr_modules_config:
  dashboard:
    port: 7001
endpoints:
  ceph_mgr:
    port:
      mgr:
        default: 7001
group_by_class: 
  {{ join_areas| to_nice_yaml| indent(2)}}
