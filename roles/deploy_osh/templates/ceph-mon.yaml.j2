---
endpoints:
  identity:
    namespace: openstack
  object_store:
    namespace: ceph
  ceph_mon:
    namespace: ceph
deployment:
  storage_secrets: true
  ceph: true
  rbd_provisioner: true
  cephfs_provisioner: false
  client_secrets: false
  rgw_keystone_user_and_endpoints: false

bootstrap:
  enabled: true

storageclass:
  rbd:
    pool: cloud.infra
  cephfs:
    provision_storage_class: false

pod:
  replicas:
{% for pod, rep in replicas[item.name].iteritems() %}
    {{pod}}: {%if cp_mode == "single" %}1{%else%}{{rep}}{%endif%}

{% endfor %}
  resources:
    enabled: {{pod_resources}}
    mgr:
      requests:
        memory: "50Mi"
        cpu: "250m"
      limits:
        memory: "256Mi"
        cpu: "1000m"
    checkdns:
      requests:
        memory: "5Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"
    mon:
      requests:
        memory: "50Mi"
        cpu: "250m"
      limits:
        memory: "256Mi"
        cpu: "1000m"
    mon_check:
      requests:
        memory: "5Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"
    tests:
      requests:
        memory: "10Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"
    jobs:
      image_repo_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      bootstrap:
        limits:
          memory: "1024Mi"
          cpu: "2000m"
        requests:
          memory: "128Mi"
          cpu: "500m"
      secret_provisioning:
        limits:
          memory: "1024Mi"
          cpu: "2000m"
        requests:
          memory: "128Mi"
          cpu: "500m"
      tests:
        requests:
          memory: "10Mi"
          cpu: "250m"
        limits:
          memory: "50Mi"
          cpu: "500m"
    rbd_provisioner:
      requests:
        memory: "5Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"

conf:
  features:
    mds: false
  ceph:
    global:
      fsid: "{{ceph_fs_id}}"
      bluestore_block_db_size: "{{bluestore_block_db_size}}"
      bluestore_block_wal_size: "{{bluestore_block_wal_size}}"
      mon_osd_full_ratio: "{{mon_osd_full_ratio}}"
      mon_osd_nearfull_ratio: "{{mon_osd_nearfull_ratio}}"
      mon_max_pg_per_osd: {{mon_max_pg_per_osd}}
  rgw_ks:
    enabled: true
  pool:
    crush:
      tunables: {{tunables}}
    target:
      osd: {{target_osds}}
      pg_per_osd: {{pg_per_osd}}
    spec:
      # RBD pool
      - name: {{infra.pool_name}}
        application: rbd
        replication: {{replication}}
        percent_total_data: {{infra_pool_percent}}
      # Cinder pools
{% if join_areas.enabled %}
{% for area in join_areas.areas %}
      - name: {{cinder.pool_name}}.{{area.name}}
        application: cinder-volume
        replication: {{replication}}
        percent_total_data: {{area.cinder_pool_percent}}
        crush_rule: {{area.name}}_rule
      - name: {{cinder.backup_pool_name}}.{{area.name}}
        application: cinder-backup
        replication: {{replication}}
        percent_total_data: {{cinder_backup_pool_percent}}
        crush_rule: {{area.name}}_rule
{% endfor %}
{% else %}
      - name: {{cinder.pool_name}}
        application: cinder-volume
        replication: {{replication}}
        percent_total_data: {{cinder_pool_percent}}
      - name: {{cinder.backup_pool_name}}
        application: cinder-backup
        replication: {{replication}}
        percent_total_data: {{cinder_backup_pool_percent}}
{% endif %}
      # Glance pool
      - name: {{glance.pool_name}}
        application: glance-image
        replication: {{replication}}
        percent_total_data: {{glance_pool_percent}}
      # Nova ephimeral pool
      - name: {{nova.ephimeral_pool_name}}
        application: nova
        replication: {{replication}}
        percent_total_data: {{nova_ephimeral_pool_percent}}
      # RadosGW pools
      - name: .rgw.root
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.control
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.data.root
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.gc
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.log
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.intent-log
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.meta
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.usage
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.keys
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.email
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.swift
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.uid
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.buckets.extra
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.buckets.index
        application: rgw
        replication: {{replication}}
        percent_total_data: 3
      - name: default.rgw.buckets.data
        application: rgw
        replication: {{replication}}
        percent_total_data: {{rgw_pool_percent}}

  storage:
    mon:
      directory: /var/lib/ceph/mon

ceph_mgr_enabled_modules:
  - restful
  - status
  - prometheus
  - balancer

# Manager default port conflict with cassandra(TungstenFabric)
ceph_mgr_modules_config:
  dashboard:
    port: 7001
endpoints:
  ceph_mgr:
    port:
      mgr:
        default: 7001

group_by_class: 
  enabled: {{join_areas.enabled}}
  areas:
{% for area in join_areas.areas %}
  - name: {{area.name}}
{% endfor %}

manifests:
  cronjob_checkPGs: true
