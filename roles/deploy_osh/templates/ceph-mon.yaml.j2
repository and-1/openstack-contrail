{%- set osd_pairs = [] %}
{%- for host in groups['osd-nodes'] %}
{%- set host_osd_jour = hostvars[host]['osd_jour'] %}
{%- set class = namespace(area="") %}
{%- if join_areas.enabled %}
{%- for area in join_areas.areas %}
{%- if area.name+'-osd-node' in hostvars[host].group_names %}
{%- set class.area = area.name %}
{%- endif %}
{%- endfor %}
{%- endif %}
{%- set disks = hostvars[host]['ansible_local']['osd_disks'] %}
{%- for disk in disks %}
{%- do osd_pairs.append(disk+"-"+host_osd_jour+"-"+class.area) %}
{%- endfor %}
{%- endfor %}
---
endpoints:
  identity:
    namespace: openstack
  object_store:
    namespace: ceph
  ceph_mon:
    namespace: ceph
deployment:
  storage_secrets: true
  ceph: true
  rbd_provisioner: true
  cephfs_provisioner: false
  client_secrets: false
  rgw_keystone_user_and_endpoints: false

bootstrap:
  enabled: true

storageclass:
  rbd:
    pool: cloud.infra
  cephfs:
    provision_storage_class: false

conf:
  features:
    mds: false
  ceph:
    global:
      fsid: "{{ceph_fs_id}}"
      bluestore_block_db_size: "{{bluestore_block_db_size}}"
      bluestore_block_wal_size: "{{bluestore_block_wal_size}}"
      mon_osd_full_ratio: "{{mon_osd_full_ratio}}"
      mon_osd_nearfull_ratio: "{{mon_osd_nearfull_ratio}}"
      mon_max_pg_per_osd: {{mon_max_pg_per_osd}}
  rgw_ks:
    enabled: true
  pool:
    crush:
      tunables: {{tunables}}
    target:
      osd: {{target_osds}}
      pg_per_osd: {{pg_per_osd}}
    spec:
      # RBD pool
      - name: {{infra.pool_name}}
        application: rbd
        replication: {{replication}}
        percent_total_data: {{infra_pool_percent}}
      # Cinder pools
{% if join_areas.enabled %}
{% for area in join_areas.areas %}
      - name: {{cinder.pool_name}}.{{area.name}}
        application: cinder-volume
        replication: {{replication}}
        percent_total_data: {{area.cinder_pool_percent}}
        crush_rule: {{area.name}}_rule
      - name: {{cinder.backup_pool_name}}.{{area.name}}
        application: cinder-backup
        replication: {{replication}}
        percent_total_data: {{cinder_backup_pool_percent}}
        crush_rule: {{area.name}}_rule
{% endfor %}
{% else %}
      - name: {{cinder.pool_name}}
        application: cinder-volume
        replication: {{replication}}
        percent_total_data: {{cinder_pool_percent}}
      - name: {{cinder.backup_pool_name}}
        application: cinder-backup
        replication: {{replication}}
        percent_total_data: {{cinder_backup_pool_percent}}
{% endif %}
      # Glance pool
      - name: {{glance.pool_name}}
        application: glance-image
        replication: {{replication}}
        percent_total_data: {{glance_pool_percent}}
      # Nova ephimeral pool
      - name: {{nova.ephimeral_pool_name}}
        application: nova
        replication: {{replication}}
        percent_total_data: {{nova_ephimeral_pool_percent}}
      # RadosGW pools
      - name: .rgw.root
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.control
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.data.root
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.gc
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.log
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.intent-log
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.meta
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.usage
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.keys
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.email
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.swift
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.users.uid
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.buckets.extra
        application: rgw
        replication: {{replication}}
        percent_total_data: 0.1
      - name: default.rgw.buckets.index
        application: rgw
        replication: {{replication}}
        percent_total_data: 3
      - name: default.rgw.buckets.data
        application: rgw
        replication: {{replication}}
        percent_total_data: {{rgw_pool_percent}}

  storage:
    mon:
      directory: /var/lib/ceph/mon
  overrides:
    ceph_osd:
      labels:
{% for osd in osd_pairs|unique %}
{% set params = osd.split('-') %}
       - label: 
{% if join_areas.enabled %}
           key: d{{params[0]}}-j{{params[1]}}-{{params[2]}}
{% else %}
           key: d{{params[0]}}-j{{params[1]}}
{% endif %}
           values: 
           - "enabled"
         #exclusive: false
         conf:
           storage:
             osd:
               - data:
                   type: block-bluestore
                   location: /dev/{{params[0]}}
                 journal:
                   type: block-bluestore
                   location: /dev/{{params[1]}}
{% endfor %}

ceph_mgr_enabled_modules:
  - restful
  - status
  - prometheus
  - balancer

# Manager default port conflict with cassandra(TungstenFabric)
ceph_mgr_modules_config:
  dashboard:
    port: 7001
endpoints:
  ceph_mgr:
    port:
      mgr:
        default: 7001

group_by_class: 
  enabled: {{join_areas.enabled}}
  areas:
{% for area in join_areas.areas %}
  - name: {{area.name}}
{% endfor %}

manifests:
  cronjob_checkPGs: true
