network:
  backend:
    - opencontrail
  novncproxy:
    ingress:
      public: false

dependencies:
  dynamic:
    targeted:
      opencontrail:
        compute:
          daemonset: []
{% if not multi_region.master %}
  static:
    api:
      services:
        - endpoint: internal
          service: oslo_messaging
        - endpoint: internal
          service: oslo_db
        - endpoint: public
          service: compute_metadata
    bootstrap:
      services:
        - endpoint: internal
          service: compute
    cell_setup:
      services:
        - endpoint: internal
          service: oslo_messaging
        - endpoint: internal
          service: oslo_db
        - endpoint: internal
          service: compute
    service_cleaner:
      services:
        - endpoint: internal
          service: oslo_messaging
        - endpoint: internal
          service: oslo_db
        - endpoint: internal
          service: compute
    conductor:
      services:
        - endpoint: internal
          service: oslo_messaging
        - endpoint: internal
          service: oslo_db
        - endpoint: internal
          service: compute
    consoleauth:
      services:
        - endpoint: internal
          service: oslo_messaging
        - endpoint: internal
          service: oslo_db
        - endpoint: internal
          service: compute
    ks_endpoints:
      services: null
    ks_service:
      services: null
    ks_user:
      services: null
    scheduler:
      services:
        - endpoint: internal
          service: oslo_messaging
        - endpoint: internal
          service: oslo_db
        - endpoint: internal
          service: compute
{% endif %}
bootstrap:
  enabled: true
  structured:
    {{nova.bootstrap | to_nice_yaml | indent(4)}}

console:
  # serial | spice | novnc | none
  console_kind: spice
  spice:
    compute:
      # IF blank, search default routing interface
      server_proxyclient_interface: {{k8s_iface}}
    proxy:
      # IF blank, search default routing interface
      server_proxyclient_interface: {{k8s_iface}}

manifests:
  service_novncproxy: false
  service_ingress_novncproxy: false
  ingress_novncproxy: false

conf:
  ceph:
    cinder:
      secret_uuid: {{rbd_secret_uuid}}
  nova:
    DEFAULT:
      ram_allocation_ratio: 1.0
      disk_allocation_ratio: 1.0
      cpu_allocation_ratio: 3.0
      osapi_compute_workers: {{nova.workers.osapi_compute}}
      metadata_workers: {{nova.workers.metadata}}
      allow_resize_to_same_host: true
    conductor:
      workers: {{nova.workers.conductor}}
    scheduler:
      driver: filter_scheduler
    filter_scheduler:
      available_filters: nova.scheduler.filters.all_filters
      enabled_filters: RetryFilter, AvailabilityZoneFilter, ComputeFilter, ComputeCapabilitiesFilter, ImagePropertiesFilter, ServerGroupAntiAffinityFilter, ServerGroupAffinityFilter
    libvirt:
      images_type: rbd
      images_rbd_pool: {{nova.ephimeral_pool_name}}
      rbd_secret_uuid: {{rbd_secret_uuid}}
      cpu_mode: {{nova.cpu_mode}}
      virt_type: {{nova.virt_type}}
    neutron:
      metadata_proxy_shared_secret: "{{metadata_secret_key}}"
    database:
      db_max_retries: -1
    api_database:
      db_max_retries: -1
    cell0_database:
      db_max_retries: -1
{% if join_areas.enabled %}
    cinder:
      cross_az_attach: 'false'
{% endif %}
  rabbitmq:
    policies:
      - vhost: "nova"
        name: "ha_ttl_nova"
        definition:
          #mirror messges to other nodes in rmq cluster
          ha-mode: "all"
          ha-sync-mode: "automatic"
          #70s
          message-ttl: 70000
        priority: 0
        apply-to: all
        pattern: '^(?!amq\.).*'
# Will be added in future for domain instead role
#  policy:
#    az_by_role: "(not None:%(availability_zone)s and project_id:%(project_id)s and (( 'private':%(availability_zone)s and not role:dmz) or ('dmz':%(availability_zone)s and role:dmz))) or is_admin:True"
#    os_compute_api:servers:create: rule:az_by_role
  wsgi_placement: |
{%- raw %}
    Listen 0.0.0.0:{{ tuple "placement" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}
    LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined
    LogFormat "%{X-Forwarded-For}i %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" proxy
    SetEnvIf X-Forwarded-For "^.*\..*\..*\..*" forwarded
    CustomLog /dev/stdout combined env=!forwarded
    CustomLog /dev/stdout proxy env=forwarded
    <VirtualHost *:{{ tuple "placement" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}>
{% endraw %}
        WSGIDaemonProcess placement-api processes={{placement.processes}} threads={{placement.threads}} user=nova group=nova display-name=%{GROUP}
        WSGIProcessGroup placement-api
        WSGIScriptAlias / /var/www/cgi-bin/nova/nova-placement-api
        WSGIApplicationGroup %{GLOBAL}
        WSGIPassAuthorization On
        <IfVersion >= 2.4>
          ErrorLogFormat "%{cu}t %M"
        </IfVersion>
        ErrorLog /dev/stdout
        SetEnvIf X-Forwarded-For "^.*\..*\..*\..*" forwarded
        CustomLog /dev/stdout combined env=!forwarded
        CustomLog /dev/stdout proxy env=forwarded
    </VirtualHost>
    Alias /placement /var/www/cgi-bin/nova/nova-placement-api
    <Location /placement>
        SetHandler wsgi-script
        Options +ExecCGI
        WSGIProcessGroup placement-api
        WSGIApplicationGroup %{GLOBAL}
        WSGIPassAuthorization On
    </Location>

pod:
  replicas:
{% for pod, rep in replicas[item.name].iteritems() %}
    {{pod}}: {%if cp_mode == "single" %}1{%else%}{{rep}}{%endif%}

{% endfor %}
  lifecycle:
    upgrades:
      deployments:
        revision_history: 3
        pod_replacement_strategy: RollingUpdate
        rolling_update:
          max_unavailable: 1
          max_surge: 3
      daemonsets:
        pod_replacement_strategy: RollingUpdate
        compute:
          enabled: true
          min_ready_seconds: 0
          max_unavailable: 1
    disruption_budget:
      metadata:
        min_available: 0
      placement:
        min_available: 0
      osapi:
        min_available: 0
    termination_grace_period:
      metadata:
        timeout: 30
      placement:
        timeout: 30
      osapi:
        timeout: 30
  resources:
    enabled: {{pod_resources}}
    compute:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    compute_ironic:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    api_metadata:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    placement:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    api:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    conductor:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    consoleauth:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    scheduler:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    ssh:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    novncproxy:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    spiceproxy:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    jobs:
      bootstrap:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      rabbit_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_drop:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_endpoints:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_service:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_user:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      tests:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      cell_setup:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      service_cleaner:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      image_repo_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
