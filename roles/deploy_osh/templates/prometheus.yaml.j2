pod:
  replicas:
    prometheus: 2
  lifecycle:
    upgrades:
      revision_history: 3
      pod_replacement_strategy: RollingUpdate
      rolling_update:
        max_unavailable: 1
        max_surge: 3
    termination_grace_period:
      prometheus:
        timeout: 30
  resources:
    enabled: {{pod_resources}}
    prometheus:
      limits:
        memory: "1024Mi"
        cpu: "2000m"
      requests:
        memory: "128Mi"
        cpu: "500m"
    jobs:
      image_repo_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      tests:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
storage:
  requests:
    storage: {{prometheus.storage}}
endpoints:
  ldap:
    path:
      default: "/ou=People,dc={{global_domain_suffix.split('.')|join(',dc=')}}"
{%- raw %}
conf:
  prometheus:
    rules:
      postgresql: null
      elasticsearch:
        groups:
        - name: elasticsearch.rules
          rules:
          - alert: es_high_process_open_files_count
            expr: sum(elasticsearch_process_open_files_count) by (host) > 64000
            for: 10m
            labels:
              severity: warning
            annotations:
              description: 'Elasticsearch at {{ $labels.host }} has more than 64000 process open file count.'
              summary: 'Elasticsearch has a very high process open file count.'
          - alert: es_high_process_cpu_percent
            expr: elasticsearch_process_cpu_percent > 95
            for: 10m
            labels:
              severity: warning
            annotations:
              description: 'Elasticsearch at {{ $labels.instance }} has high process cpu percent of {{ $value }}.'
              summary: 'Elasticsearch process cpu usage is more than 95 percent.'
          - alert: es_fs_usage_high
            expr: (100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes) / elasticsearch_filesystem_data_size_bytes) > 80
            for: 10m
            labels:
              severity: warning
            annotations:
              description: 'Elasticsearch at {{ $labels.instance }} has filesystem usage of {{ $value }}.'
              summary: 'Elasticsearch filesystem usage is high.'
          - alert: es_unassigned_shards
            expr: elasticsearch_cluster_health_unassigned_shards > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              description: 'Elasticsearch has {{ $value }} unassigned shards.'
              summary: 'Elasticsearch has unassigned shards and hence a unhealthy cluster state.'
          - alert: es_cluster_health_timed_out
            expr: elasticsearch_cluster_health_timed_out > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              description: 'Elasticsearch cluster health status call timedout {{ $value }} times.'
              summary: 'Elasticsearch cluster health status calls are timing out.'
          - alert: es_cluster_health_status_alert
            expr: elasticsearch_cluster_health_status{color="green"} != 1
            for: 10m
            labels:
              severity: warning
            annotations:
              description: 'Elasticsearch cluster health status is not green. One or more shards or replicas are unallocated.'
              summary: 'Elasticsearch cluster health status is not green.'
          - alert: es_cluster_health_too_few_nodes_running
            expr: elasticsearch_cluster_health_number_of_nodes < 3
            for: 10m
            labels:
              severity: warning
            annotations:
              description: 'There are only {{$value}} < 3 ElasticSearch nodes running'
              summary: 'ElasticSearch running on less than 3 nodes'
          - alert: es_cluster_health_too_few_data_nodes_running
            expr: elasticsearch_cluster_health_number_of_data_nodes < 3
            for: 10m
            labels:
              severity: warning
            annotations:
              description: 'There are only {{$value}} < 3 ElasticSearch data nodes running'
              summary: 'ElasticSearch running on less than 3 data nodes'
      prometheus_exporters:
        groups:
        - name: prometheus_exporters.rules
          rules:
          - alert: prom_exporter_ceph_unavailable
            expr: absent(ceph_health_status)
            for: 10m
            labels:
              severity: warning
            annotations:
              description: Ceph exporter is not collecting metrics or is not available for past 10 minutes
              title: Ceph exporter is not collecting metrics or is not available
          - alert: prom_exporter_openstack_unavailable
            expr: absent(openstack_exporter_cache_refresh_duration_seconds)
            for: 10m
            labels:
              severity: warning
            annotations:
              description: Openstack exporter is not collecting metrics or is not available for past 10 minutes
              title: Openstack exporter is not collecting metrics or is not available
          - alert: prom_exporter_mariadb_unavailable
            expr: absent(mysql_up)
            for: 10m
            labels:
              severity: warning
            annotations:
              description: MariaDB exporter is not collecting metrics or is not available for past 10 minutes
              title: MariaDB exporter is not collecting metrics or is not available
          - alert: prom_exporter_kube_state_metrics_unavailable
            expr: absent(kube_node_info)
            for: 10m
            labels:
              severity: warning
            annotations:
              description: kube-state-metrics exporter is not collecting metrics or is not available for past 10 minutes
              title: kube-state-metrics exporter is not collecting metrics or is not available
          - alert: prom_exporter_node_unavailable
            expr: absent(node_uname_info)
            for: 10m
            labels:
              severity: warning
            annotations:
              description: node exporter is not collecting metrics or is not available for past 10 minutes
              title: node exporter is not collecting metrics or is not available
          - alert: prom_exporter_calico_unavailable
            expr: absent(felix_host)
            for: 10m
            labels:
              severity: warning
            annotations:
              description: Calico exporter is not collecting metrics or is not available for past 10 minutes
              title: Calico exporter is not collecting metrics or is not available
          - alert: prom_exporter_elasticsearch_unavailable
            expr: absent(elasticsearch_cluster_health_status)
            for: 10m
            labels:
              severity: warning
            annotations:
              description: Elasticsearch exporter is not collecting metrics or is not available for past 10 minutes
              title: Elasticsearch exporter is not collecting metrics or is not available
          - alert: prom_exporter_fluentd_unavailable
            expr: absent(fluentd_up)
            for: 10m
            labels:
              severity: warning
            annotations:
              description: Fluentd exporter is not collecting metrics or is not available for past 10 minutes
              title: Fluentd exporter is not collecting metrics or is not available
      kubernetes:
        groups:
        - name: kubernetes.rules
          rules:
          - alert: kube_statefulset_replicas_unavailable
            expr: kube_statefulset_status_replicas < kube_statefulset_replicas
            for: 5m
            labels:
              severity: page
            annotations:
              description: 'statefulset {{$labels.statefulset}} has {{$value}} replicas, which is less than desired'
              summary: '{{$labels.statefulset}}: has inssuficient replicas.'
          - alert: daemonsets_misscheduled
            expr: kube_daemonset_status_number_misscheduled > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              description: 'Daemonset {{$labels.daemonset}} is running where it is not supposed to run'
              summary: 'Daemonsets not scheduled correctly'
          - alert: daemonsets_not_scheduled
            expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              description: '{{ $value }} of Daemonset {{$labels.daemonset}} scheduled which is less than desired number'
              summary: 'Less than desired number of daemonsets scheduled'
          - alert: deployment_replicas_unavailable
            expr: kube_deployment_status_replicas_unavailable > 0
            for: 10m
            labels:
              severity: page
            annotations:
              description: 'deployment {{$labels.deployment}} has {{$value}} replicas unavailable'
              summary: '{{$labels.deployment}}: has inssuficient replicas.'
          - alert: rollingupdate_deployment_replica_less_than_spec_max_unavailable
            expr: kube_deployment_status_replicas_available - kube_deployment_spec_strategy_rollingupdate_max_unavailable < 0
            for: 10m
            labels:
              severity: page
            annotations:
              description: 'deployment {{$labels.deployment}} has {{$value}} replicas available which is less than specified as max unavailable during a rolling update'
              summary: '{{$labels.deployment}}: has inssuficient replicas during a rolling update.'
          - alert: job_status_failed
            expr: kube_job_status_failed > 0
            for: 10m
            labels:
              severity: page
            annotations:
              description: 'Job {{$labels.exported_job}} is in failed status'
              summary: '{{$labels.exported_job}} has failed status'
          - alert: pod_status_pending
            expr: kube_pod_status_phase{phase="Pending"} == 1
            for: 10m
            labels:
              severity: page
            annotations:
              description: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} has been in pending status for more than 10 minutes'
              summary: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} in pending status'
          - alert: pod_error_image_pull
            expr: kube_pod_container_status_waiting_reason {reason="ErrImagePull"} == 1
            for: 10m
            labels:
              severity: page
            annotations:
              description: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} has an Image pull error for more than 10 minutes'
              summary: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} in error status'
          - alert: pod_status_error_image_pull
            expr: kube_pod_container_status_waiting_reason {reason="ErrImagePull"} == 1
            for: 10m
            labels:
              severity: page
            annotations:
              description: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} has an Image pull error for more than 10 minutes'
              summary: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} in error status'
          - alert: pod_error_crash_loop_back_off
            expr: kube_pod_container_status_waiting_reason {reason="CrashLoopBackOff"} == 1
            for: 10m
            labels:
              severity: page
            annotations:
              description: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} has an CrashLoopBackOff  error for more than 10 minutes'
              summary: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} in error status'
          - alert: replicaset_missing_replicas
            expr:  kube_replicaset_spec_replicas -  kube_replicaset_status_ready_replicas > 0
            for: 10m
            labels:
              severity: page
            annotations:
              description: 'Replicaset {{$labels.replicaset}} is missing desired number of replicas for more than 10 minutes'
              summary: 'Replicaset {{$labels.replicaset}} is missing replicas'
          - alert: pod_container_terminated
            expr: kube_pod_container_status_terminated_reason{reason=~"OOMKilled|Error|ContainerCannotRun"} > 0
            for: 10m
            labels:
              severity: page
            annotations:
              description: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} has a container terminated for more than 10 minutes'
              summary: 'Pod {{$labels.pod}} in namespace {{$labels.namespace}} in error status'
          - alert: volume_claim_capacity_high_utilization
            expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.80
            for: 5m
            labels:
              severity: page
            annotations:
              description: 'volume claim {{$labels.persistentvolumeclaim}} usage has exceeded 80% of total capacity'
              summary: '{{$labels.persistentvolumeclaim}} usage has exceeded 80% of total capacity.'
      basic_linux:
        groups:
        - name: basic_linux.rules
          rules:
          - alert: node_filesystem_full_80percent
            expr: sort(node_filesystem_free{device!~"ramfs|tmpfs|debugfs|cgroup|cgroup2|configfs|fusectl|proc|pstore|securityfs"} < node_filesystem_size{device!~"ramfs|tmpfs|debugfs|cgroup|cgroup2|configfs|fusectl|proc|pstore|securityfs"}
              * 0.2) / 1024 ^ 3
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} device {{$labels.device}} on {{$labels.mountpoint}}
                got less than 10% space left on its filesystem.'
              summary: '{{$labels.alias}}: Filesystem is running out of space soon.'
          - alert: node_filesystem_full_in_4h
            expr: predict_linear(node_filesystem_free{device!~"ramfs|tmpfs|debugfs|cgroup|cgroup2|configfs|fusectl|proc|pstore|securityfs"}[1h], 4 * 3600) <= 0
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} device {{$labels.device}} on {{$labels.mountpoint}}
                is running out of space of in approx. 4 hours'
              summary: '{{$labels.alias}}: Filesystem is running out of space in 4 hours.'
          - alert: node_filedescriptors_full_in_3h
            expr: predict_linear(node_filefd_allocated[1h], 3 * 3600) >= node_filefd_maximum
            for: 20m
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} is running out of available file descriptors
                in approx. 3 hours'
              summary: '{{$labels.alias}} is running out of available file descriptors in
                3 hours.'
          - alert: node_load1_90percent
            expr: node_load1 / ON(alias) count(node_cpu{mode="system"}) BY (alias) >= 0.9
            for: 1h
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} is running with > 90% total load for at least
                1h.'
              summary: '{{$labels.alias}}: Running on high load.'
          - alert: node_cpu_util_90percent
            expr: 100 - (avg(irate(node_cpu{mode="idle"}[5m])) BY (alias) * 100) >= 90
            for: 1h
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} has total CPU utilization over 90% for at least
                1h.'
              summary: '{{$labels.alias}}: High CPU utilization.'
          - alert: node_ram_using_90percent
            expr: node_memory_MemFree + node_memory_Buffers + node_memory_Cached < node_memory_MemTotal
              * 0.1
            for: 30m
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} is using at least 90% of its RAM for at least
                30 minutes now.'
              summary: '{{$labels.alias}}: Using lots of RAM.'
          - alert: node_swap_using_80percent
            expr: node_memory_SwapTotal - (node_memory_SwapFree + node_memory_SwapCached)
              > node_memory_SwapTotal * 0.8
            for: 10m
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} is using 80% of its swap space for at least
                10 minutes now.'
              summary: '{{$labels.alias}}: Running out of swap soon.'
          - alert: node_high_cpu_load
            expr: node_load15 / on(alias) count(node_cpu{mode="system"}) by (alias) >= 0
            for: 1m
            labels:
              severity: warning
            annotations:
              description: '{{$labels.alias}} is running with load15 > 1 for at least 5 minutes: {{$value}}'
              summary: '{{$labels.alias}}: Running on high load: {{$value}}'
          - alert: node_high_memory_load
            expr: (sum(node_memory_MemTotal) - sum(node_memory_MemFree + node_memory_Buffers
              + node_memory_Cached)) / sum(node_memory_MemTotal) * 100 > 85
            for: 1m
            labels:
              severity: warning
            annotations:
              description: Host memory usage is {{ humanize $value }}%. Reported by
                instance {{ $labels.instance }} of job {{ $labels.job }}.
              summary: Server memory is almost full
          - alert: node_high_storage_load
            expr: (node_filesystem_size{mountpoint="/"} - node_filesystem_free{mountpoint="/"})
              / node_filesystem_size{mountpoint="/"} * 100 > 85
            for: 30s
            labels:
              severity: warning
            annotations:
              description: Host storage usage is {{ humanize $value }}%. Reported by
                instance {{ $labels.instance }} of job {{ $labels.job }}.
              summary: Server storage is almost full
          - alert: node_high_swap
            expr: (node_memory_SwapTotal - node_memory_SwapFree) < (node_memory_SwapTotal
              * 0.4)
            for: 1m
            labels:
              severity: warning
            annotations:
              description: Host system has a high swap usage of {{ humanize $value }}. Reported
                by instance {{ $labels.instance }} of job {{ $labels.job }}.
              summary: Server has a high swap usage
          - alert: node_high_network_drop_rcv
            expr: node_network_receive_drop{device!="lo"} > 3000
            for: 30s
            labels:
              severity: warning
            annotations:
              description: Host system has an unusally high drop in network reception ({{
                humanize $value }}). Reported by instance {{ $labels.instance }} of job {{
                $labels.job }}
              summary: Server has a high receive drop
          - alert: node_high_network_drop_send
            expr: node_network_transmit_drop{device!="lo"} > 3000
            for: 30s
            labels:
              severity: warning
            annotations:
              description: Host system has an unusally high drop in network transmission ({{
                humanize $value }}). Reported by instance {{ $labels.instance }} of job {{
                $labels.job }}
              summary: Server has a high transmit drop
          - alert: node_high_network_errs_rcv
            expr: node_network_receive_errs{device!="lo"} > 3000
            for: 30s
            labels:
              severity: warning
            annotations:
              description: Host system has an unusally high error rate in network reception
                ({{ humanize $value }}). Reported by instance {{ $labels.instance }} of job
                {{ $labels.job }}
              summary: Server has unusual high reception errors
          - alert: node_high_network_errs_send
            expr: node_network_transmit_errs{device!="lo"} > 3000
            for: 30s
            labels:
              severity: warning
            annotations:
              description: Host system has an unusally high error rate in network transmission
                ({{ humanize $value }}). Reported by instance {{ $labels.instance }} of job
                {{ $labels.job }}
              summary: Server has unusual high transmission errors
          - alert: node_network_conntrack_usage_80percent
            expr: sort(node_nf_conntrack_entries{job="node-exporter"} > node_nf_conntrack_entries_limit{job="node-exporter"}  * 0.8)
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.instance}} has network conntrack entries of {{ $value }} which is more than 80% of maximum limit'
              summary: '{{$labels.instance}}: available network conntrack entries are low.'
          - alert: node_entropy_available_low
            expr: node_entropy_available_bits < 300
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.instance}} has available entropy bits of {{ $value }} which is less than required of 300'
              summary: '{{$labels.instance}}: is low on entropy bits.'
          - alert: node_hwmon_high_cpu_temp
            expr: node_hwmon_temp_crit_celsius*0.9 - node_hwmon_temp_celsius < 0 OR node_hwmon_temp_max_celsius*0.95 - node_hwmon_temp_celsius < 0
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} reports hwmon sensor {{$labels.sensor}}/{{$labels.chip}} temperature value is nearly critical: {{$value}}'
              summary: '{{$labels.alias}}: Sensor {{$labels.sensor}}/{{$labels.chip}} temp is high: {{$value}}'
          - alert: node_vmstat_paging_rate_high
            expr: irate(node_vmstat_pgpgin[5m]) > 80
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} has a memory paging rate of change higher than 80%: {{$value}}'
              summary: '{{$labels.alias}}: memory paging rate is high: {{$value}}'
          - alert: node_xfs_block_allocation_high
            expr: 100*(node_xfs_extent_allocation_blocks_allocated_total{job="node-exporter", instance=~"172.17.0.1.*"} / (node_xfs_extent_allocation_blocks_freed_total{job="node-exporter", instance=~"172.17.0.1.*"} + node_xfs_extent_allocation_blocks_allocated_total{job="node-exporter", instance=~"172.17.0.1.*"})) > 80
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} has xfs allocation blocks higher than 80%: {{$value}}'
              summary: '{{$labels.alias}}: xfs block allocation high: {{$value}}'
          - alert: node_network_bond_slaves_down
            expr: node_net_bonding_slaves - node_net_bonding_slaves_active > 0
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{ $labels.master }} is missing {{ $value }} slave interface(s).'
              summary: 'Instance {{ $labels.instance }}: {{ $labels.master }} missing {{ $value }} slave interface(s)'
          - alert: node_numa_memory_used
            expr: 100*node_memory_numa_MemUsed / node_memory_numa_MemTotal > 80
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} has more than 80% NUMA memory usage: {{ $value }}'
              summary: '{{$labels.alias}}: has high NUMA memory usage: {{$value}}'
          - alert: node_ntp_clock_skew_high
            expr: abs(node_ntp_drift_seconds) > 2
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.alias}} has time difference of more than 2 seconds compared to NTP server: {{ $value }}'
              summary: '{{$labels.alias}}: time is skewed by : {{$value}} seconds'
          - alert: node_disk_read_latency
            expr: (rate(node_disk_read_time_ms[5m]) / rate(node_disk_reads_completed[5m])) > 10
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.device}} has a high read latency of {{ $value }}'
              summary: 'High read latency observed for device {{ $labels.device }}'
          - alert: node_disk_write_latency
            expr: (rate(node_disk_write_time_ms[5m]) / rate(node_disk_writes_completed[5m])) > 10
            for: 5m
            labels:
              severity: page
            annotations:
              description: '{{$labels.device}} has a high write latency of {{ $value }}'
              summary: 'High write latency observed for device {{ $labels.device }}'
    scrape_configs:
      global:
        scrape_interval: 60s
        evaluation_interval: 60s
      scrape_configs:
        # NOTE(srwilkers): The job definition for Prometheus should always be
        # listed first, so we can inject the basic auth username and password
        # via the endpoints section
        - job_name: 'prometheus-metrics'
          kubernetes_sd_configs:
          - role: endpoints
          scrape_interval: 60s
          relabel_configs:
          - source_labels:
              - __meta_kubernetes_service_name
            action: keep
            regex: "prom-metrics"
          - source_labels:
              - __meta_kubernetes_service_annotation_prometheus_io_scrape
            action: keep
            regex: true
          - source_labels:
              - __meta_kubernetes_service_annotation_prometheus_io_scheme
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels:
              - __meta_kubernetes_service_annotation_prometheus_io_path
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels:
              - __address__
              - __meta_kubernetes_service_annotation_prometheus_io_port
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels:
              - __meta_kubernetes_namespace
            action: replace
            target_label: kubernetes_namespace
          - source_labels:
              - __meta_kubernetes_service_name
            action: replace
            target_label: instance
          - source_labels:
              - __meta_kubernetes_service_name
            action: replace
            target_label: kubernetes_name
          - source_labels:
              - __meta_kubernetes_service_name
            target_label: job
            replacement: ${1}
        - job_name: kubelet
          scheme: https
          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          kubernetes_sd_configs:
          - role: node
          scrape_interval: 45s
          relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels:
              - __meta_kubernetes_node_name
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics
          - source_labels:
              - __meta_kubernetes_node_name
            action: replace
            target_label: kubernetes_io_hostname
          # Scrape config for Kubelet cAdvisor.
          #
          # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
          # (those whose names begin with 'container_') have been removed from the
          # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
          # retrieve those metrics.
          #
          # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
          # HTTP endpoint; use "replacement: /api/v1/nodes/${1}:4194/proxy/metrics"
          # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with
          # the --cadvisor-port=0 Kubelet flag).
          #
          # This job is not necessary and should be removed in Kubernetes 1.6 and
          # earlier versions, or it will cause the metrics to be scraped twice.
        - job_name: 'kubernetes-cadvisor'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
          - role: node

          relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels:
              - __meta_kubernetes_node_name
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          metric_relabel_configs:
          - source_labels:
              - __name__
            regex: 'container_network_tcp_usage_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_tasks_state'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_network_udp_usage_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_memory_failures_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_cpu_load_average_10s'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_cpu_system_seconds_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_cpu_user_seconds_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_inodes_free'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_inodes_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_io_current'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_io_time_seconds_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_io_time_weighted_seconds_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_read_seconds_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_reads_merged_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_reads_merged_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_reads_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_sector_reads_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_sector_writes_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_write_seconds_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_writes_bytes_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_writes_merged_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_fs_writes_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_last_seen'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_memory_cache'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_memory_failcnt'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_memory_max_usage_bytes'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_memory_rss'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_memory_swap'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_memory_usage_bytes'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_network_receive_errors_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_network_receive_packets_dropped_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_network_receive_packets_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_network_transmit_errors_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_network_transmit_packets_dropped_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_network_transmit_packets_total'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_spec_cpu_period'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_spec_cpu_shares'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_spec_memory_limit_bytes'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_spec_memory_reservation_limit_bytes'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_spec_memory_swap_limit_bytes'
            action: drop
          - source_labels:
              - __name__
            regex: 'container_start_time_seconds'
            action: drop
          # Scrape config for API servers.
          #
          # Kubernetes exposes API servers as endpoints to the default/kubernetes
          # service so this uses `endpoints` role and uses relabelling to only keep
          # the endpoints associated with the default/kubernetes service using the
          # default named port `https`. This works for single API server deployments as
          # well as HA API server deployments.
        - job_name: 'apiserver'
          kubernetes_sd_configs:
          - role: endpoints
          scrape_interval: 45s
          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https
          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            # insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          # Keep only the default/kubernetes service endpoints for the https port. This
          # will add targets for each API server which Kubernetes adds an endpoint to
          # the default/kubernetes service.
          relabel_configs:
          - source_labels:
              - __meta_kubernetes_namespace
              - __meta_kubernetes_service_name
              - __meta_kubernetes_endpoint_port_name
            action: keep
            regex: default;kubernetes;https
          metric_relabel_configs:
          - source_labels:
              - __name__
            regex: 'apiserver_admission_controller_admission_latencies_seconds_bucket'
            action: drop
          - source_labels:
              - __name__
            regex: 'rest_client_request_latency_seconds_bucket'
            action: drop
          - source_labels:
              - __name__
            regex: 'apiserver_response_sizes_bucket'
            action: drop
          - source_labels:
              - __name__
            regex: 'apiserver_admission_step_admission_latencies_seconds_bucket'
            action: drop
          - source_labels:
              - __name__
            regex: 'apiserver_admission_controller_admission_latencies_seconds_count'
            action: drop
          - source_labels:
              - __name__
            regex: 'apiserver_admission_controller_admission_latencies_seconds_sum'
            action: drop
          - source_labels:
              - __name__
            regex: 'apiserver_request_latencies_summary'
            action: drop
        # Scrape config for service endpoints.
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: If the metrics are exposed on a different port to the
        # service then set this appropriately.
        - job_name: 'openstack-exporter'
          kubernetes_sd_configs:
          - role: endpoints
          scrape_interval: 60s
          relabel_configs:
          - source_labels:
              - __meta_kubernetes_service_name
            action: keep
            regex: "openstack-metrics"
          - source_labels:
              - __meta_kubernetes_service_annotation_prometheus_io_scrape
            action: keep
            regex: true
          - source_labels:
              - __meta_kubernetes_service_annotation_prometheus_io_scheme
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels:
              - __meta_kubernetes_service_annotation_prometheus_io_path
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels:
              - __address__
              - __meta_kubernetes_service_annotation_prometheus_io_port
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels:
              - __meta_kubernetes_namespace
            action: replace
            target_label: kubernetes_namespace
          - source_labels:
              - __meta_kubernetes_service_name
            action: replace
            target_label: instance
          - source_labels:
              - __meta_kubernetes_service_name
            action: replace
            target_label: kubernetes_name
          - source_labels:
              - __meta_kubernetes_service_name
            target_label: job
            replacement: ${1}
        - job_name: 'kubernetes-service-endpoints'
          kubernetes_sd_configs:
          - role: endpoints
          scrape_interval: 60s
          relabel_configs:
          - source_labels:
              - __meta_kubernetes_service_name
            action: drop
            regex: '(openstack-metrics|prom-metrics|ceph-mgr)'
          - source_labels:
              - __meta_kubernetes_service_annotation_prometheus_io_scrape
            action: keep
            regex: true
          - source_labels:
              - __meta_kubernetes_service_annotation_prometheus_io_scheme
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels:
              - __meta_kubernetes_service_annotation_prometheus_io_path
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels:
              - __address__
              - __meta_kubernetes_service_annotation_prometheus_io_port
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels:
              - __meta_kubernetes_namespace
            action: replace
            target_label: kubernetes_namespace
          - source_labels:
              - __meta_kubernetes_service_name
            action: replace
            target_label: kubernetes_name
          - source_labels:
              - __meta_kubernetes_service_name
            target_label: job
            replacement: ${1}
        # Example scrape config for pods
        #
        # The relabeling allows the actual pod scrape endpoint to be configured via the
        # following annotations:
        #
        # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the
        # pod's declared ports (default is a port-free target if none are declared).
        - job_name: 'kubernetes-pods'
          kubernetes_sd_configs:
          - role: pod
          relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
        - job_name: calico-etcd
          kubernetes_sd_configs:
          - role: service
          scrape_interval: 20s
          relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - action: keep
            source_labels:
              - __meta_kubernetes_service_name
            regex: "calico-etcd"
          - action: keep
            source_labels:
              - __meta_kubernetes_namespace
            regex: kube-system
            target_label: namespace
          - source_labels:
              - __meta_kubernetes_pod_name
            target_label: pod
          - source_labels:
              - __meta_kubernetes_service_name
            target_label: service
          - source_labels:
              - __meta_kubernetes_service_name
            target_label: job
            replacement: ${1}
          - source_labels:
              - __meta_kubernetes_service_label
            target_label: job
            regex: calico-etcd
            replacement: ${1}
          - target_label: endpoint
            replacement: "calico-etcd"
        - job_name: ceph-mgr
          kubernetes_sd_configs:
          - role: endpoints
          scrape_interval: 20s
          relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - action: keep
            source_labels:
              - __meta_kubernetes_service_name
            regex: "ceph-mgr"
          - source_labels:
              - __meta_kubernetes_service_port_name
            action: drop
            regex: 'ceph-mgr'
          - action: keep
            source_labels:
              - __meta_kubernetes_namespace
            regex: ceph
            target_label: namespace
          - action: keep
            source_labels:
              - __meta_kubernetes_pod_container_port_number
            regex: 9283
          - source_labels:
              - __meta_kubernetes_pod_name
            target_label: pod
          - source_labels:
              - __meta_kubernetes_service_name
            target_label: service
          - source_labels:
              - __meta_kubernetes_service_name
            target_label: job
            replacement: ${1}
          - source_labels:
              - __meta_kubernetes_service_label
            target_label: job
            regex: ceph-mgr
            replacement: ${1}
          - target_label: endpoint
            replacement: "ceph-mgr"
{% endraw %}
