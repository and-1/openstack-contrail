storage: ceph
pod:
  replicas:
{% for pod, rep in replicas[item.name].iteritems() %}
    {{pod}}: {%if cp_mode == "single" %}1{%else%}{{rep}}{%endif%}

{% endfor %}
  lifecycle:
    upgrades:
      deployments:
        revision_history: 3
        pod_replacement_strategy: RollingUpdate
        rolling_update:
          max_unavailable: 1
          max_surge: 3
    disruption_budget:
      api:
        min_available: 0
  resources:
    enabled: {{pod_resources}}
    api:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    scheduler:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    volume:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    jobs:
      volume_usage_audit:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      bootstrap:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      rabbit_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_drop:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      clean:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      backup_storage_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      storage_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_endpoints:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_service:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_user:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      tests:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      image_repo_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"

conf:
  cinder:
    DEFAULT:
      backup_driver: cinder.backup.drivers.ceph
      enabled_backends: "{{cinder.volume_backend_name}}"
      os_region_name: {{region}}
      backup_ceph_chunk_size: "134217728"
      backup_ceph_stripe_unit: 0
      backup_ceph_stripe_count: 0
      backup_ceph_pool: {{cinder.backup_pool_name}}
      restore_discard_excess_bytes: "true"
      osapi_volume_workers: {{cinder.workers.osapi_volume}}
      scheduler_default_filters: "AvailabilityZoneFilter,CapacityFilter,CapabilitiesFilter"
    database:
      db_max_retries: -1
      
  backends:
    rbd1: null
    {{cinder.volume_backend_name}}:
      volume_backend_name: {{cinder.volume_backend_name}}
      volume_driver: cinder.volume.drivers.rbd.RBDDriver
      rbd_pool: {{cinder.pool_name}}
      rbd_ceph_conf: "/etc/ceph/ceph.conf"
      rbd_flatten_volume_from_snapshot: false
      rbd_max_clone_depth: 5
      rbd_store_chunk_size: 4
      rados_connect_timeout: -1
      rbd_user: cinder
      rbd_secret_uuid: {{rbd_secret_uuid}}

  rally_tests:
    run_tempest: false
    tests:
      CinderVolumes.create_and_list_volume_backups:
        - args:
            size: 1
            detailed: True
            do_delete: True
            create_volume_kwargs: {}
            create_backup_kwargs: {}
          runner:
            type: "constant"
            times: 1
            concurrency: 1
          sla:
            failure_rate:
              max: 0

  rabbitmq:
    #NOTE(rk760n): adding rmq policy to mirror messages from notification queues and set expiration time for the ones
    policies:
      - vhost: "cinder"
        name: "ha_ttl_cinder"
        definition:
          #mirror messges to other nodes in rmq cluster
          ha-mode: "all"
          ha-sync-mode: "automatic"
          #70s
          message-ttl: 70000
        priority: 0
        apply-to: all
        pattern: '^(?!amq\.).*'

{% if cinder_backup_to_region %}
backup:
  external_ceph_rbd:
    enabled: true
    admin_keyring: {{ext_ceph_cinder.keyring}}
    conf:
      global:
        mon_host: {{ext_ceph_cinder.mons}}
      osd:
      client:
        rdb_cache = true
{% endif %}

{% if not multi_region.master %}
dependencies:
  static:
    api:
      services:
        - endpoint: internal
          service: oslo_db
    backup:
      services:
        - endpoint: internal
          service: volume
    bootstrap:
      services:
        - endpoint: internal
          service: volume
    ks_endpoints:
      services: null
    ks_service:
      services: null
    ks_user:
      services: null
    scheduler:
      services:
        - endpoint: internal
          service: volume
    tests:
      services:
        - endpoint: internal
          service: volume
    volume:
      services:
        - endpoint: internal
          service: volume
    volume_usage_audit:
      services:
        - endpoint: internal
          service: volume
{% endif %}
